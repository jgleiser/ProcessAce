---
trigger: always_on
---

# ProcessAce – Agent Development Guidelines

This document defines how AI agents (e.g. Antigravity / Google Gemini) should generate and modify code and configuration for ProcessAce.

Core goals:

- **Maintainability** – code must be easy to read, extend, and refactor.
- **Auditability** – all important actions must be traceable via logs and versioned artifacts.
- **Process-awareness** – the application itself should support process improvement and downstream process mining.
- **Scalability** – long-running operations must use background workers, not block HTTP requests.

---

## 1. General coding principles

1. Use clear, consistent naming and small, focused modules/functions.
2. Prefer explicit over implicit behavior. Avoid “magic” side effects.
3. Write code that other engineers can understand quickly:
   - Add short, meaningful comments where intent is not obvious.
   - Avoid clever but obscure patterns.
4. Respect existing style and tooling:
   - Follow `.eslintrc` and Prettier formatting.
   - Keep files and imports organized and consistent.
5. Avoid introducing external dependencies unless they clearly add value.
   - Prefer well-known, actively maintained libraries when needed.

---

## 2. Logging and auditability

The application must produce **event-style logs** suitable for process mining and audit.

### 2.1. Structured logging

- Use **structured logs** (JSON-like fields) rather than only free-text strings.
- Every significant action should produce a log entry with at least:
  - `timestamp`
  - `event_type` (e.g. `ingestion_started`, `llm_call`, `artifact_generated`, `artifact_version_created`, `job_queued`, `job_completed`)
  - `actor` (user id, system, or `"system"` for background jobs)
  - `correlation_id` or `request_id`
  - `project_id` / `process_id` if applicable
- Logs should be easy to ingest into process mining tools later.

When adding new features, always ask:

> “What event(s) should be logged so this can be reconstructed later?”

### 2.2. LLM interaction logging

For every LLM call, log a **LLM interaction event** with:

- `event_type = "llm_call"`
- `llm_provider` (e.g. `openai`, `azure`, `local`)
- `llm_model`
- `prompt_type` or `use_case` (e.g. `evidence_extraction`, `bpmn_generation`)
- **Prompt metadata**:
  - Either the full prompt (if allowed) or a redacted/hashed representation.
  - Always include the prompt structure and key variables.
- **Response metadata**:
  - Length of response (tokens/characters).
  - Status (success/error).
  - Any parsing/validation errors.

If prompts or responses may contain sensitive data, ensure there is a clear place in the code to apply redaction or anonymization before logging.

### 2.3. Error and exception logging

- All errors should be logged with:
  - `event_type = "error"`
  - `error_type`
  - `message`
  - `stack` (where safe)
  - Relevant IDs (user, project, artifact, job)
- Avoid swallowing errors silently.

---

## 3. Document and artifact versioning

Every generated artifact must be **versioned**, not overwritten in place.

Artifacts include at least:

- BPMN 2.0 XML models
- SIPOC tables
- RACI matrices
- Narrative documentation (Markdown/HTML)

### 3.1. Version model

When implementing storage for artifacts, ensure:

- Each artifact has:
  - `artifact_id` (stable identifier)
  - `version` (integer or semver-like)
  - `artifact_type` (e.g. `bpmn`, `sipoc`, `raci`, `doc`)
  - `created_at`, `created_by`
  - Optional `source_evidence_version` (link to the evidence state)
- New generations or edits **create a new version**:
  - Do **not** delete or overwrite previous versions.
- Support retrieving:
  - The latest version.
  - A specific version.
  - A version history list.

### 3.2. Version change logging

Whenever an artifact version is created or updated, log an event:

- `event_type = "artifact_version_created"` or `"artifact_version_updated"`
- `artifact_type`
- `artifact_id`
- `version`
- `created_by`
- Optionally:
  - `previous_version`
  - `llm_call_id` if generated by LLM.

---

## 4. Data flow and separation of concerns

When adding new code, keep these separation principles:

1. **Ingestion vs. analysis vs. generation**
   - Ingestion: handling raw files (audio/video/docs/images) and storing them.
   - Analysis: transcription, parsing, extraction into the Process Evidence model.
   - Generation: producing BPMN/SIPOC/RACI/docs from normalized evidence.
   - Each stage should be represented by separate modules/services and separate log events.

2. **LLM abstraction**
   - Do not call a specific LLM vendor directly from feature code.
   - Always go through the **LLM abstraction layer**:
     - One central module handles provider URLs, API keys, models, and HTTP.
     - Feature modules only define prompts and expected response shapes.

3. **Config vs. code**
   - Provider URLs, API keys, and model names should be loaded from configuration/environment, not hard-coded.
   - Feature-specific parameters (e.g. max tokens, temperature) should be centralized where possible.

---

## 5. Long-running tasks and background workers

Processing large files and multi-step workflows must not block HTTP requests.

### 5.1. Async pattern

When implementing long-running operations (e.g. video transcription, full pipeline analysis, artifact regeneration):

- **Do not** perform them directly in the request/response cycle.
- Instead:
  - API endpoint:
    - Validates input.
    - Persists necessary metadata.
    - Enqueues a job in the job queue (e.g. Redis-backed).
    - Returns `202 Accepted` with:
      - `job_id`
      - A status endpoint/URL.
  - Background worker:
    - Picks up the job.
    - Performs the heavy work in stages (ingestion → analysis → generation).
    - Emits structured log events for each step.
    - Updates job status (`pending`, `in_progress`, `completed`, `failed`).

The UI should be able to poll or subscribe to the job status instead of expecting immediate results.

### 5.2. Job and worker logging

For jobs and workers, emit events like:

- `job_queued`
- `job_started`
- `job_step_started` / `job_step_completed`
- `job_completed`
- `job_failed`

Each event should include:

- `job_id`
- `job_type` (e.g. `process_evidence`, `transcribe_media`)
- `project_id` / `process_id` (if relevant)
- `actor` (usually `"system"` for workers)
- Error details for failures.

---

## 6. Testing and validation

When generating or modifying code:

1. Prefer adding **unit tests** or simple integration tests for:
   - Transformation functions (evidence → artifacts).
   - Versioning logic.
   - Logging/event emission helpers.
   - Job queue handlers (where practical).

2. For BPMN/SIPOC/RACI generation:
   - Add validations when possible:
     - BPMN schema validation (or structural checks).
     - Simple consistency checks (e.g. no orphan tasks).
   - Ensure failures are logged with `event_type = "validation_error"`.

3. Avoid tests that depend directly on external LLMs:
   - Use mocks/fakes for LLM responses in tests.
   - Keep real LLM calls behind a boundary that can be stubbed.

---

## 7. Files and structure expectations for agents

When an agent creates or modifies files, try to respect this structure:

- **Root**
  - `src/` – application code
    - `api/` – HTTP/API handlers
    - `services/` – business logic (generation, evidence handling)
    - `workers/` – background job processors
    - `llm/` – LLM abstraction and prompts
    - `logging/` – logging utilities and event types
    - `models/` – data models (artifacts, evidence, jobs, users, etc.)
  - `tests/` – test files mirroring `src/` structure
  - `docs/` – documentation (including this file and `architecture.md`)

This structure may evolve, but keep it clean and consistent.

---

## 8. Non-goals (for now)

When adding new features, avoid:

- Implementing heavy, speculative optimizations prematurely.
- Introducing tightly coupled UI/backend logic (keep them separated).
- Bypassing logging, versioning, or the job queue for “quick hacks”.
- Implementing multi-tenant features that conflict with the current open edition scope without explicit design.

If a shortcut is necessary for experimentation, it should be clearly marked as such and not merged into `main` without a path to a maintainable, auditable implementation.

---

By following these guidelines, agents help ensure that ProcessAce remains a reliable tool for process improvement, with clear traceability of how processes are interpreted, transformed, and documented over time.
